---
title: "Atlas Tutorial Part I"
output:
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
```


[server]: http://18.198.246.143:10002


## Intro
In the first part of the tutorial, you will 
1. learn how to install Atlas, 
2. init a new project and 
3. run it. 

Everything is done in the command line.



### Setup

**If you work on Windows don't open this webpage in Microsoft edge, it doesn't display all the interactive part correctly**

1. Connect to the server on this [link][server]
2. Login with the credentials you received before.
3. Go to the terminal

![In the upper right corner. Click on new, then Terminal](images/go_to_terminal_jupyter.png)

It is as if you would connect to the server of your institution.

### Conda


The only dependency for metagenome atlas is the *conda package manager*. It can easily be installed with the miniconda package.

On the server, we have already installed it. 

<!--
You only need to activate it by running:
```{bash, eval=FALSE}
/opt/miniconda3/bin/conda init
exec bash
```

To have access to all up-to-date bioinformatic packages you should add tell conda too look for them in the *conda-forge* and the *bioconda* channel in addition to the default.


```{bash eval=F}
conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge
```

These two channels are community-driven efforts to make many software packages installable as easy as possible.
-->


Different programs need often different versions of python/R or other packages. It can become a nightmare to install different tools with conflicting dependencies. Conda allows you to encapsulate each software into its own *environment*.

--> Create an conda environment, then activate it.
```{bash eval=F}
mamba create --yes -n atlasenv
```

You should see a `(atlasenv)` at the beginning of the bash line.

### Install atlas

Now let's install metagenome-atlas.

```{r setupa, echo=FALSE}
question("What is the command to install metagenome-atlas",
  answer("conda install metagenome-atlas",correct = TRUE, message="mamba is a faster alternative to conda, but both work."),
  answer("mamba install metagenome-atlas", correct = TRUE),
  answer("snakemake install metagenome-atlas")
)
```

--> Run the fastest command to install metagenome-atlas.



## Initialisation
### Start a new project

Run `atlas --help`

```{r init1, echo=FALSE}
question("What is the subcommand that you will run to start a new project?",
  answer("atlas download", message="All databases and software is installed on the fly. Download is optional."),
  answer("atlas init",correct = TRUE, message = "Run the command with the '--help' to see what attributes you need"),
  answer("atlas run")
)
```


To start a new project you need the path to the fastq files of your metagenome samples (You analyze all samples together). We have provided you with two samples of test data in the folder `test_reads`.

The other parameter you should set is the database directory. This should point to a path where you can store the >150GB databases and software installed automatically by metagenome-atlas. Ideally, this is also a shared location with your colleges. For the Tutorial, we will simply use `databases` folder which already exists.

--> run the init command:

```{bash, eval=FALSE}
atlas init --db-dir databases test_reads
```

```{r init, echo=FALSE}
question("What files did atlas create directory?",
  answer("test_reads"),
  answer("databases",message="the database folder actually existed before"),
  answer("config.yaml", correct = TRUE),
  answer("atlas"),
  answer("samples.tsv",correct = TRUE)
)
```



### Configure


--> Have a look at the `samples.tsv` with `cat samples.tsv`.
Check if the names of the samples are inferred correctly. Samples should be alphanumeric names and cam be dash delimited. The `BinGroup` parameter can be used to activate co-binning. All samples in the same BinGroup are used mapped against each other. However, as it doesn't make sense to use this feature for less than three samples we don't use it and let the default.

Let's check the `config.yaml` file. Open it in the integrated file browser on the jupyter page (The first page you entered after the login). It contains many parameters to configure the pipeline. They are explained in the comments or more in detail in the documentation.


```{r config, echo=FALSE}
message="The 'assembly_memory' uses up to 250GB"
question("With the default configuration. How much memory would be used maximally? ",
  answer("250 MB", message="Memory units are in GB"),
  answer(" 60 GB",message= message),
  answer("250 GB", correct=T),
  answer(" 60 MB",message= message)
)
```

You can also see that this amount of memory would be needed for 48h with 8 CPUs. When I meant that metagenomics is resource-intensive I was not joking. Luckily, most institutions have a high-performance computing system with allow you to run such calculations.


If you scroll down to `contaminant_references`, you can already see that the Phix genome is added as a contaminant. The Phix is a virus that is frequently used as a control for sequencing. Even if you haven't used it in your sequencing there is some chance that some reads might swim around in the sequencer.

Let's add a host genome that should be removed during the decontamination step.
You should find a `human_genome.fasta` in your database folder.

<!--
First, find out the absolute path to the human genome, then add it to the config file in the section `contaminant_references`.
-->

Adapt the contaminant_references as follows:
```
contaminant_references:
  PhiX: /path/to/databases/phiX174_virus.fa
  human: /oath/to/databases/human_genome.fasta
```

Don't just copy the snippet above you need to replace `/path/to/` with the correct absolute path. It's the same for both contaminant references.

Pay attention that there are two spaces at the beginning of the line. Finally, save the file.

## Run atlas





### Dry Run

Before Running the pipeline, which can take more than a day it is always recommended to do a dry-run. This simulates an execution and checks if there are any errors in the config file or elsewhere.

--> Run `atlas run --help` to see how to do a dry-run.
--> Call the run command with the dry-run option and `assembly` as workflow. Also specify the working directory. 

**In case you missed the dry-run parameter the use `CTR+C` to stop the run.**

The dry-run command takes a while and then it shows a list of all the steps that would be executed.

```{r steps, echo=FALSE}
question("How many steps would be executed by atlas?",
  answer("4"),
  answer("128", correct = TRUE, message="Remember the number."), #128 for genomes
  answer("174")
)
```


This command runs all the steps in the atlas pipeline for two samples. You can see how it would scale for more samples.

### Ressources

If you run metagenome-atlas on your cluster you should set up a cluster profile, as described in the documentation. This allows atlas to submitt each step to the cluster while specifying the number of cores and memory, not bad! 

For the demo, we chose a sipler way and run atlas on a single node. If you run atlas on a single node you have to tell him how much memory and how many cores are available. On our server we have 20gb and two cores.


--> Run the following command.

```{bash eval=F}
atlas run assembly --resources mem=10 java_mem=7 --jobs 2
```

### Automatic installation of software

You should see something like:
```{bash, eval=FALSE}
Creating conda environment /usr/local/envs/atlasenv/lib/python3.6/site-packages/atlas/rules/../envs/report.yaml...
Downloading and installing remote packages.
```


In the beginning, atlas installs all the software it needs in conda environments. This can take quite some time but has to be done only once. You can easily run metagenome-atlas on a longrunning cluster job and come back some hours later. But wait -- stay here!  For the tutorial, we have already installed all except one conda environment so the

Once the conda environments are installed atlas will start with the steps of the pipeline.



## Stop and go


You might wonder what happens if the server crashes during the long execution of Atlas.

â€“> press `CTR + C.`

This simulates a system crash. The pipeline should stop and do some cleanup.

Now, use &#8593; and `Enter` to run the `atlas run assembly` command again. How many steps are now be executed?
Do you see, that metagenome-atlas can continue to run the pipeline form the where it stopped? There are even checkpoints during the assembly from which it can continue.


## QC + Assembly

While the pipeline is finishing you can already answer the flowing questions?

The output of the qc, and the assembly sub-workflow are summarized in two reports.

The `reports/QC_report.html` gives a graphical report on the most important numbers.


The dataset used for the Tutorial is a very small one, here you can see the [QC report](https://metagenome-atlas.readthedocs.io/en/latest/_static/QC_report.html) of a bigger run.

<!--
```{r qcreport2, echo=FALSE}
message= "Sample F26 has lost many reads during the quality filtering, maybe it would make sense to drop it altogether."
question("In the bigger run, are all samples of good quality? ",
  answer("yes", message=message),
  answer("no",message=message, correct = TRUE)
)
```
-->

### Assembly report

Once the assembly sub-workflow is finished, it will again produce a report `reports/assembly_report.html`. 

--> Copy it to your computer and open it with a webbrowser

Are you happy with the fraction of assembled reads?

### Sample specific output files
There are also many files produced for each sample. Once the assembly workflow is finished try to find the quality-controlled reads (.fastq) for sample1?

```{r qcfastq, echo=FALSE}
message="We have paired-reads. The third file contains the reads that lost their mate during the equality control. They are seamlessly integrated in the pipeline."
question("How many qc fastq files are there per sample?",
  answer("1", message = message),
  answer("2", message = message),
  answer("3",correct = TRUE, message = message)
)
```




## Binning
### Introduction

We try to reconstruct genomes from the metagenome.
This is done by grouping together contigs which we think belong to the same genome.
These groups are called **bins**, and if we really think it is a genome then we call it **MAG for metagenome-assembled genome**


![](https://merenlab.org/momics/03-reconstructing-genomes-from-metagenomes.gif){width=15cm}

*Here is a short animation from a class of Prof. Eren https://merenlab.org/momics/*


By default, we use the automatic binners metabat2 and maxbin2 and then a bin refining with DAS-Tool.
Both binners are based on the sequence composition and the coverage profile in one sample. To get the coverage profile we first need to align the reads to the assembly.

### Run the binning workflow

For the binning workflow we exclude one step which uses > 40GB of memory. 


Run the command
```{bash eval=F}
atlas run binning --resources mem=10 java_mem=7 --jobs 2 --omit-from download_checkm_data
```



Once the pipeline has finished, let's look at the output.

First, open the contigs file for sample 2 (.fasta file).

```{r nsample, echo=FALSE}
question("What is the name of the first contig? ",
  answer("sample_0"),
  answer("sample2_1"),
  answer("sample2_0",correct = TRUE, message='It is also the longest contig in the file.')
)
```




Each binner produces a `cluster_attribution.tsv` which provides the link between contig and bin.
Have a look at the one from sample 2 produced by both binners, by running:
```{bash eval=F}
head sample2/binning/*/cluster_attribution.tsv
```

```{r qbin1, echo=FALSE}
question("In which bin is the longest contig of sample 2? ",
    answer("sample2_metabat_1",correct=TRUE),
    answer("sample2_metabat_2"),
    answer("sample2_maxbin_1",correct=TRUE),
    answer("sample2_maxbin_2"),
    answer("No clue.", message="the longest contig as stated above is 'sample2_0'")
)
```

Maxbin produces also a summary file. Have a look.

```{bash eval=F}
cat sample2/binning/maxbin/sample2.summary
```

```{r qbin2, echo=FALSE}
question("What is the completeness of the worst bin? ",
  answer("< 90%"),
  answer("< 60%",correct=TRUE)
)
```

### Bin refining

The DAS Tool takes the predictions of both binners and tries to find a harmonization with the best result, meaning the highest quality. For example: The bin `sample2_metabat_1` and `sample2_maxbin_1` contain both the longest contig `sample2_0` and many others. The DAS tool chooses the better of the two.  For this, the DAS Tool also estimates the quality of the bins.

Have a look at the plot in: `sample2/binning/DASTool/sample2_DASTool_scores.pdf`

It shows how DAS tool estimates the quality of both biners and its results.

```{r qbin3, echo=FALSE}
question("Which binner produces the better results?",
  answer("metabat",correct=TRUE, message = "This is something we see throughout, e.g. also in the publication of DAS Tool."),
  answer("maxbin"),
  answer("DAS tool",message="DAS Tool is actually not a binner, rather a bin refiner.")
)
```


```{r qbin4, echo=FALSE}
question("Is the quality estimate of DAS Tool as bad as the one from maxbin for the lower quality bins?",
  answer("No, DAS Tool thinks bins are not as bad as maxbin.",correct=TRUE, message = "The reason is they use a different set of marker genes"),
  answer("Yes, DAS Tool estimates bins are also of low quality.", message = "No, the reason is they use a different set of marker genes")
)
```

### Quality assessment
It is not easy to find a set of marker genes for each genome, especially if the genome belongs to unknown species. And different sets yield different results. Usually, the final quality estimates are made with BUSCO or CheckM, which adapt the marker gene set to better estimate the quality.

Unfortunately, this step cannot be run until the end as the genome quality estimation needs more than 100GB of RAM.

But, you can look [here](https://rawcdn.githack.com/metagenome-atlas/Tutorial/31f89f0bdf476ab34b7e52ebd68ef14e20b9c677/media/bin_report_DASTool.html) at the final output of the binning report.

```{r qbin5, echo=FALSE}
question("Was the low-quality estimation of maxbin correct at the end? Assuming that the checkM quality estimation in the report is closest to the truth?",
  answer("maxbin quality estimation was way too low",correct=TRUE, message = "The species in this dataset have very small genomes, this may cause the low-quality estimation."),
  answer("maxbin quality estimation was ok")
)
```




```{r qbin6, echo=FALSE}
question("How many different species are there in both samples? ",
  answer("2"),
  answer("3",correct=TRUE, message = ),
  answer("5", message = "some genomes from sample 1 and 2 belong to the same species")
)
```

The *genomes* sub-workflow of Atlas combines the binning results from different samples and produces a non-redundant set of MAGs. The workflow also quantifies the genomes in all the samples and annotates them with a better taxonomy.

## End

Because of the computational and time limits we won't run atlas until the end. It won't any way not be very interesting for this small dataset.  Instead, you will analyze the output of a more interesting project in part 2 of the Tutorial.



You can no go [Part 2 of the Tutorial](https://metagenome-atlas.shinyapps.io/Part2). On the flowing pages are some extra exercises about the atlas gene-catalog workflow.



## Extra

### Gene catalog



```{bash eval=F}
atlas run genecatalog --profile Demo --working-dir workdir
```

Have a look at the output of the genecatalog wokflow.
```{bash,eval=FALSE}
head Genecatalog/gene_catalog.faa
```

Count how many genes are there, by counting the fasta headers.
```{bash,eval=FALSE}
grep -c '^>' Genecatalog/gene_catalog.faa
```
