---
title: "Atlas Tutorial Part I"
output:
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
```

## Setup
### Setup conda

The only dependency for metagenome atlas is the *conda package manager*. It can easily be installed with the miniconda package. To have access to all up-to-date bioinformatic packages you should add tell conda too look for them in the *conda-forge* and the *bioconda* channel in addition to the default.


```{bash eval=F}
conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge
```

These two channels are community-driven efforts to make many software packages installable as easy as possible.

Different programs need often different versions of python/R or other packages. It can become a nightmare to install different tools with conflicting dependencies. Conda allows you to encapsulate each software into its own *environment*.

--> Run the following command
```{bash eval=F}
conda create --yes -n atlasenv
```
--> activate the environment

You should see a `(atlasenv)` at the beginning of the bash line.

### Instal atlas

Now let's install metagenome-atlas.

```{r setupa, echo=FALSE}
question("What is the command to isntall metagenome-atlas",
  answer("conda install metagenome-atlas",correct = TRUE, message="mamba is a faster alternative to conda, but both work."),
  answer("mamba install metagenome-atlas", correct = TRUE),
  answer("snakemake install metagenome-atlas")
)
```

--> Run the command to install metagenome-atlas.

Run `atlas --help`
Do you get a message, the atlas is installed correctly.

```{r init, echo=FALSE}
question("What is the subcommand that you will run to start a new project?",
  answer("atlas download"),
  answer("atlas init",correct = TRUE, message = "Run the command with the '--help' to see what attributes you need"),
  answer("atlas run")

)
```

## Initialisation
### Start a new project


Run `atlas init --help`

To start a new metagenome project you need the fastq files of a metagenome of your samples (You analyze all samples together). We have provided you with two samples of test data in the folder `test_reads`.

The other parameter important in the init command is the `--db-dir` argument. This should point to a path where you can store the >100GB databases and software installed automatically by metagenome-atlas. Ideally, this is also a shared location with your colleges. For the Tutorial, we will simply use the `databases` folder which already exists.

Provide also a working directory (e.g. `workdir`) where all the work will be done and all the output will be placed.


--> Run the init command with the options described above.


```{r init2, echo=FALSE}
question("What files did atlas create in the working directory?",
  answer("test_reads"),
  answer("databases",message="the database folder actually existed before"),
  answer("config.yaml", correct = TRUE),
  answer("atlas"),
  answer("samples.tsv",correct = TRUE),
  answer("Help, I don't know which comand I should run",message = "\nThe command would be: 'atlas init --working-dir workdir --db-dir databases test_reads'")
)
```

### Configure


Have a look at the `samples.tsv` with the integrated text editor or the nano command. Check if the names of the samples are inferred correctly. Samples should be alphanumeric names and cam be dash delimited. The `BinGroup` parameter can be used to activate co-binning. All samples in the same BinGroup are used mapped against each other. However, as it doesn't make sense to use this feature for less than three samples we don't use it and let the default.

Let's check the `config.yaml` file. It contains many parameters to configure the pipeline. They are explained in the comments or more in detail in the documentation.


```{r config, echo=FALSE}
message="The 'assembly_memory' uses up to 250GB"
question("With the default configuration. How much memory would be used maximally? ",
  answer("250 MB", message="Memory units are in GB"),
  answer(" 60 GB",message= message),
  answer("250 GB", correct=T),
  answer(" 60 MB",message= message)
)
```

Let's add a host genome that should be removed during the decontamination step.
You should find a `human_genome.fasta` in your database folder.
First, find out the absolute path to the human genome, then add it to the config file in the section `contaminant_references`.

```
contaminant_references:
  PhiX: /path/to/databases/phiX174_virus.fa
  human: /path/to/databases/human_genome.fasta
```
Don't just copy the snippet above you need to replace `/path/to/` with the correct absolute path. It's the same for both contaminant references.
Pay attention that there are two spaces at the beginning of the line. Finally, save the file.

## Run atlas

### Dry Run

Before Running the pipeline, which can take more than a day it is always recommended to do a dry-run. This simulates an execution and checks if there are any errors in the config file or elsewhere.

--> Run `atlas run --help` to see how to do a dry-run and how to specify the working directory.
--> Call the run command with the dry-run and the working Directory parameter.


**In case you missed the dry-run parameter the use `CTR+C` to stop the run.**

The dry-run command takes a while and then it shows a list of all the steps that would be executed.

```{r dry, echo=FALSE}
question("How many steps would be executed by atlas?",
  answer("4"),
  answer("145", correct = TRUE),
  answer("174")
)
```

This command runs all the steps in the atlas pipeline for two staples.



## Quality control
### Run QC

To understand better what atlas does we, begin to run only the quality control sub-workflow (QC). For your project, you don't need to run QC separately. You can directly run all steps with `atlas run all` from the beginning.

If you run metagenome-atlas on your cluster (or cloud) you should set up a cluster profile, as described in the documentation. For the demo, I made a profile called `Demo`. **Important: always run atlas with the `--profile Demo` parameters otherwise the Tutorial server can crash.** As before, we need also to specify also the working directory to the run command.

--> Run the following command (Note the `qc`):

```{bash eval=F}
atlas run qc --profile Demo --working-dir workdir
```
 See what happens.


### Automatic installation of software

You should see something like:
```{bash, eval=FALSE}
Creating conda environment /usr/local/envs/atlasenv/lib/python3.6/site-packages/atlas/rules/../envs/required_packages.yaml...
Downloading and installing remote packages.
```

At the beginning, atlas installs all the software it needs in conda environments. This can take quite some time but has to be done only once. You can easily run metagenome-atlas in a screen and come back some hours later. But wait -- stay here!  For the tutorial, we have already installed all except one conda environment so the

Once the conda environments are installed atlas will start with the steps of the pipeline. On a cluster, each step can be automatically submitted to the cluster. In this case, you can run many jobs or steps in parallel. Even when you don't want to submit jobs to the cluster you can run metagenome-atlas on a server with high memory. In this case, atlas can also run many jobs in parallel. The Demo profile limits the memory usage so that one step is executed after the other.

### Output of QC
Once the QC sub-workflow has finished, check what files are listed as input to the `qc` rule.

```{bash, eval=FALSE}
[Wed Nov 25 11:13:45 2020]
localrule qc:
    input: sample2/sequence_quality_control/finished_QC, sample1/sequence_quality_control/finished_QC, stats/read_counts.tsv, stats/insert_stats.tsv, stats/read_length_stats.tsv, reports/QC_report.html

```
This are the main output files of the QC workflow for all samples.

--> Try to open the `stats/read_counts.tsv` with the integrated text editor. Which sample has more reads?

The `reports/QC_report.html` gives a graphical report on the most important numbers.
The dataset used for the Tutorial is a very small one, here you can see the [QC report](https://metagenome-atlas.readthedocs.io/en/latest/_static/QC_report.html) of a bigger run.


```{r qcreport2, echo=FALSE}
message= "Sample F26 has lost many reads during the quality filtering, maybe it would make sense to drop it altogether."
question("In the bigger run, are all samples of good quality? ",
  answer("yes", message=message),
  answer("no",message=message, correct = TRUE)
)
```


There are also many files produced for each sample. Can you find the quality-controlled reads (.fastq) for sample1?

```{r qcfastq, echo=FALSE}
message="We have paired-reads. The third file contains the reads that lost their mate during the equality control. They are seamlessly integrated in the pipeline."
question("How many qc fastq files are there per sample?",
  answer("1", message = message),
  answer("2", message = message),
  answer("3",correct = TRUE, message = message)
)
```


## Assembly
### Stop and go

As already mentioned, the assembly is the most time and memory consuming step.
You might wonder what happens if the server crashes during the long execution of Atlas.

Run the dry run command for assembly.

```{bash, eval=F}
atlas run assembly --profile Demo --working-dir workdir --dryrun
```

```{r dryass, echo=FALSE}
question("How many steps would now be executed by atlas?",
  answer("4"),
  answer("174"),
  answer("42", correct=TRUE , "The meaning of everything.")
)
```
–> run the command without the dry-run.

Wait until one or two steps have finished then. –> press `CTR + C.`

This simulates a system crash. The pipeline should stop and do some cleanup.

Now run again the dry-run command. How many steps would now be executed by atlas?
Do you see, that metagenome-atlas can continue to run the pipeline form the where it stopped? There are even checkpoints during the assembly from which it can continue.




### Assembly output

--> Run the assembly workflow until the end.

While the assembly runs can you answer the flowing questions?
```{r demoass, echo=FALSE}
message="The Demo profile adapts the memory requirement to the system."
question("How much memory are used for the assembly step?",
  answer("250 GB",message=message),
  answer("2 GB", correct = TRUE, message=message),
  answer("60 GB",message=message)
)
```


Once the assembly sub-workflow is finished, it will again produce a report `reports/assembly_report.html`. You can open [this one](https://metagenome-atlas.readthedocs.io/en/latest/_static/assembly_report.html) until your pipeline has finished.

The assembly report shows different statistics about the length and number of the contigs.  

Download the extended statistics at the bottom of the report, and open the text file with Excel or a text editor.

```{r sizeass, echo=FALSE}
message="A quart of an E. coli genome! "
question("What is the longest contig in any sample?",
  answer("~ 1 Kbp",message=message),
  answer("~ 1 Mbp", correct = TRUE, message=message),
  answer("~ 1 Gbp",message=message)
)
```

Info: there is also the option to use scaffolding with the spades assembler to combine contigs that can be linked by paired-reads.


Once the assembly workflow is finished try to find the contigs file for sample 2 (.fasta file). Open it.


```{r nsample, echo=FALSE}
question("What is the name of the first contig? ",
  answer("sample_0"),
  answer("sample2_1"),
  answer("sample2_0",correct = TRUE, message='In python we start to count from 0, it is also the longest contig in the file.')
)
```


## Binning
### Introduction
Now comes the most interesting.

Start by running
```{bash eval=F}
cd workdir
atlas run binning --profile Demo --working-dir workdir
```


We try to reconstruct genomes from the metagenome.
This is done by grouping together contigs which we think belong to the same genome.
These groups are called **bins**, and if we really think it is a genome then we call it **MAG for metagenome-assembled genome**

![Animation](https://merenlab.org/momics/03-reconstructing-genomes-from-metagenomes.gif){width=15cm}


*Here is a short animation from the a class of Prof. Eren https://merenlab.org/momics/*


By default, we use the automatic binners metabat2 and maxbin2 and then a bin refining with DAS-Tool.
Both binners are based on the sequence composition and the coverage profile in one sample. To get the coverage profile we first need to align the reads to the assembly.

### Bins

Once the pipeline has finished, let's look at the output.

Each binner produces a `cluster_attribution.tsv` which provides the link between contig and bin.
Have a look at the one from sample 2 produced by both binners, by running:
```{bash eval=F}
head sample2/binning/*/cluster_attribution.tsv
```

```{r qbin1, echo=FALSE}
question("In which bin is the longest contig of sample 2? ",
    answer("sample2_metabat_1",correct=TRUE),
    answer("sample2_metabat_2"),
    answer("sample2_maxbin_1",correct=TRUE),
    answer("sample2_maxbin_2"),
    answer("No clue.", message="the longest contig as stated above is 'sample2_0'")
)
```

Maxbin produces also a summary file. Have a look.

```{bash eval=F}
cat sample2/binning/maxbin/sample2.summary
```

```{r qbin2, echo=FALSE}
question("What is the completeness of the worst bin? ",
  answer("< 90%"),
  answer("< 60%",correct=TRUE)
)
```

### Bin refining

The DAS Tool takes the predictions of both binners and tries to find a harmonization with the best result, meaning the highest quality. For example: The bin `sample2_metabat_1` and `sample2_maxbin_1` contain both the longest contig `sample2_0` and many others. The DAS tool chooses the better of the two.  For this, the DAS Tool also estimates the quality of the bins.

Have a look at the plot in: `sample2/binning/DASTool/sample2_DASTool_scores.pdf`

It shows how DAS tool estimates the quality of both biners and its results.

```{r qbin3, echo=FALSE}
question("Which binner produces the better results?",
  answer("metabat",correct=TRUE, message = "This is something we see throughout, e.g. also in the publication of DAS Tool."),
  answer("maxbin"),
  answer("DAS tool",message="DAS Tool is actually not a binner, rather a bin refiner.")
)
```


```{r qbin4, echo=FALSE}
question("Do the quality estimates of maxbin and DAS Tool agree for the lower quality bins?",
  answer("No",correct=TRUE, message = "The reason is they use a different set of marker genes"),
  answer("Yes", message = "No, the reason is they use a different set of marker genes")
)
```

### Quality assessment
It is not easy to find a set of marker genes for each genome, especially if the genome belongs to unknown species. And different sets yield different results. Usually, the final quality estimates is made with BUSCO or CheckM, which adapt the marker gene set to better estimate the quality.

Unfortunately, this step cannot be run until the end as the genome quality estimation needs more than 100GB of RAM.

But, you can look [here](binning report. ) at the final output of the binning report.
You can see that sample 1 and sample 2 produce bins or MAGs for the same species.


The *genomes* sub-workflow of Atlas combines the binning results from different samples and produces a non-redundant set of MAGs. The workflow also quantifies the genomes in all the samples and annotates them with a better taxonomy.

The final output would look like this.
This is not very interesting for this small dataset but in part 2 of the Tutorial, you will analyze the output of a more interesting project.

## End

You can no go [Part 2 of the Tutorial](https://metagenome-atlas.shinyapps.io/Part2). On the flowing pages are some extra exercises about the atlas gene-catalog workflow.

## Extra

### Gene catalog



```{bash eval=F}
atlas run genecatalog --profile Demo --working-dir workdir
```
